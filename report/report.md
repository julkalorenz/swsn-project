# Correcting misinformation on social media with a large language model

### Authors
- Jakub Kubiak, 156049
- Maciej Janicki, 156073
- Julia Lorenz, 156066

## Introduction
Misinformation is an ongoing issue on social media platforms posing significant challenges to public discourse and individual decision-making. For a long time now, misinformation detection and correction have been tasks that required informed human intervetion, which is both time-consuming and not scalable. Recent advancements in large language models (LLMs) have opened new avenues for addressing this problem, enabling automated systems to generate corrective responses to misinformation. This project aims to reproduce the method introduced in the paper *Correcting misinformation on social media with a large language model*[@zhou2024correctingmisinformationsocialmedia]. The paper proposes a novel approach to misinformation correction called MUSE, which combines search-engine querying with LLM-based summarization. The goal of this project is to implement, test, and deploy the model on a different domain.

## Related Work
As a significant issue in the digital space, misinformation has been the focus of various studies. Prior research suggests that the main cause as well as the main problem of misinformation spread is the sheer volume and speed at which information is shared on social media [@doi:10.1177/17456916221141344]. As a result, traditional methods of misinformation correction that often rely on manual fact-checking, however accurate, are not scalable, thus, inherently limited given the vast amount of content generated daily. Early work in computer science and natural language processing primarily focused on automated misinformation and fake news detection [@shu2017fake; @KHAN2021100032]. These approaches typically frame the problem as a supervised classification task. While such methods have demonstrated promising results, they are mainly designed to identify misinformation rather than to correct it. More recently, the emergence of LLMs has enabled a shift from detection oriented systems toward generative approaches [@zhou2024correctingmisinformationsocialmedia; @10.1162/tacl_a_00454] capable of producing natural language explanations and corrective responses.

## Datasets Description

### Dataset 1: Tweets Correction
The dataset is derived from the MUSE model’s GitHub repository and is used to evaluate the generated responses. It includes the text of the original tweet, the generated corrective response, and the source of the response, as well as other metadata. The response source can be one of four categories: produced by highly helpful humans, produced by humans with average helpfulness, generated by the GPT-4 model, or generated by the MUSE model.

### Dataset 2: Fake News Detection
The second dataset is derived from the FakeNewsNet repository, which compiles news articles from multiple sources, including both genuine and fake content. For this project, we focus on the subset of articles identified as fake news from the BuzzFeed website. FakeNewsNet was introduced as part of a series of studies on fake news detection on social media [@shu2017fake; @shu2017exploiting; @shu2018fakenewsnet]. Each record includes the title of the article and the corresponding main text.

### Dataset 3: Fact-checked Online Content
Final dataset also derivered from Kaggle website contains fact-checked posts from various online platforms, covering a wide range of topics between 2008 and 2022. Each entry includes the post’s title, content of the post, a truthfulness rating status, and a link to the original content.

## Method Description
The method consists of four main steps, firstly, the input content, which is a social media post is preprocessed and input into a LLM with an appropirete prompt to generate three adquete search queries. Next, there queries are used to retrieve relevant websites and sources of information from the web. Then, relevant content is extraced from the retrieved sources. Finally, the post's content along with the extracted information is fed into the LLM to generate a corrective response.

During our project we introduced an additonal step before the query generation, where we prompt the LLM to rewrite the input content to make it more suitable for query generation. This step is intended to investigate whether improving the clarity and structure of the input content can lead to more effective search queries and ultimately better corrective responses.

\begin{figure}[H]
\centering
\includegraphics[height=0.4\textheight]{images/pipeline.png}
\caption{Method Pipeline}
\end{figure}


## Evaluation Startegy

We implemented the method step by step, starting with preprocessing the content and creating the initial prompts for query generation. The preprocessing step involved cleaning the text, removing unnecessary characters, changing temporal expressions to applicable dates from content's metadata, and formatting it appropriately for input into the LLM.

\begin{figure}[H]
\centering
\includegraphics[height=0.3\textheight]{images/preprocessing.png}
\caption{Preprocessing Step}
\end{figure}

Next, we focused on the query generation step. We designed prompts to instruct the LLM to generate three relevant search queries based on the preprocessed content. We evaluated the quality of the generated queries based on their relevance to the original content and their potential to retrieve useful information manually.


\begin{figure}[H]
\centering
\includegraphics[height=0.2\textheight]{images/query_generation.png}
\caption{Query Generation Step}
\end{figure}

Following the query generation, we implemented the web scraping step to retrieve informtion form websites that were returned by the search engine using the generated queries. And using the extrected information, we combined the preprocessed content and the retrieved information into a new prompt to generate the corrective response.

\begin{figure}[H]
\centering
\includegraphics[height=0.2\textheight]{images/post_prompt_verdict.png}
\caption{Correction Generation Step}
\end{figure}

## Outcomes and Conclusions

We successfully recreated the MUSE methodology for correcting misinformation on social media using large language models. We implemented the full pipeline, including preprocessing, search query generation, web-based information retrieval, and corrective response generation. 

We applied this method to different datasets from various domains, demonstrating its versatility. The generated corrective responses were evaluated manually based on their relevance and accuracy in addressing the misinformation present in the original content.

The method's main advantages include its ability to gather information from multiple sources, providing a more comprehensive basis for corrections. Using multiple queries and sources allows for a broader perspeective on the topic being addressed. Additionally, using datasets containing different types of content, rather than solely tweets, showcases the method's adaptability to various contexts.

However, there are a few limitations to consider. The main challenge is the requirement for API keys to access LLMs and search engines, which come at a cost. The trustworthiness of the scraped content is another concern, as the quality of the corrective responses heavily depends on the reliability of the sources retrieved, while the authors of the original paper addressed this by implementing a source credibility assessment step, there is still a risk of incorporating biased or inaccurate information. What is more, LLMs tend to produce overly long and nuanced answers, which usually does not match the typical tone and style of online discourse, as well as may discourage users from reading the entire response. Finally, the method is limited by the publicly available knowledge on the web, which may not always be up-to-date or comprehensive.

To conclude, our project replicated the MUSE approach for misinformation correction using LLMs, demonstrating its effectiveness across different datasets. As misinformation spread is a viable concern in today's digital age and human fact-checking is unattainable at such scale, this method demonstrates that LLMs can be an effective supporting tool for combating misinformation on social media.


## References
MUSE GitHub repository: https://github.com/Social-Futures-Lab/MUSE/tree/main
FakeNewsNet Kaggle repository: https://www.kaggle.com/datasets/mdepak/fakenewsnet
Verified Posts Kaggle repository: https://www.kaggle.com/datasets/mattimansha/verified-posts-fact-checking-online-content